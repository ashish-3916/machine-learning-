{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive bayes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6cjMjVEI6vLeIz9Y1MXBK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish-3916/machine-learning-/blob/main/naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la2k-10kY63H"
      },
      "source": [
        "import numpy as np  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Ml40cZPpBw"
      },
      "source": [
        "def fit (x_train , y_train):\n",
        "  result = {}\n",
        "  class_values = set(y_train)  # final attribute values\n",
        "  for current_class in class_values:\n",
        "    result[current_class] = {}\n",
        "    result[\"total_data\"] = len(y_train)\n",
        "\n",
        "    # take only those rows which has this label as answer\n",
        "    current_class_rows = (y_train == current_class )\n",
        "    x_train_current = x_train[current_class_rows]\n",
        "    y_train_current = y_train[current_class_rows]\n",
        "\n",
        "    # work for every feature \n",
        "    num_features = x_train.shape[1]\n",
        "    result[current_class][\"total_count\"] = len(y_train_current)\n",
        "\n",
        "    for j in range(1 , num_features +1 ):\n",
        "      result[current_class][j] = {}\n",
        "      # all possible values these features can take \n",
        "      all_possible_values = set(x_train[: , j-1])\n",
        "      for current_value in all_possible_values:\n",
        "        result[current_class][j][current_value] = (x_train_current[: , j-1]==current_value).sum()\n",
        "\n",
        "    return result "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7StIBDD1WsgV"
      },
      "source": [
        "\n",
        "# def probabilty(dict , x , current_class):\n",
        "#   output = 1;\n",
        "#   # p(X = x | y = current_class ) * p( y = current_class)\n",
        "#   #             feature prob      *   class_prob\n",
        "#   class_prob = dict[current_class][\"total_count\"] / dict[\"total_data\"]\n",
        "#   feature_prob = 1;\n",
        "#   num_features = len(dict[current_class].keys()) - 1\n",
        "#   for j in range (1 , num_features+1):\n",
        "#     xj =  x[j-1]\n",
        "#     count_current_class_with_value_xj =  dict[current_class][j][xj] + 1 # laplace correction\n",
        "#     count_current_class = dict[current_class][\"total_count\"] + len(dict[current_class][j].keys()) #laplace correction\n",
        "#     current_p =  count_current_class_with_value_xj / count_current_class\n",
        "#     feature_prob*= current_p\n",
        "  \n",
        "#   output = feature_prob * class_prob\n",
        "\n",
        "#   return output\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpoJwZ3ub6ha"
      },
      "source": [
        "#changing to log probablities bcz in multiplication they are already so small \n",
        "\n",
        "def probabilty(dict , x , current_class):\n",
        "  # output = 1;\n",
        "  # p(X = x | y = current_class ) * p( y = current_class)\n",
        "  #             feature prob      *   class_prob\n",
        "  class_prob = np.log(dict[current_class][\"total_count\"]) - np.log(dict[\"total_data\"])\n",
        "  feature_prob = 1;\n",
        "  num_features = len(dict[current_class].keys()) - 1\n",
        "  for j in range (1 , num_features+1):\n",
        "    xj =  x[j-1]\n",
        "    count_current_class_with_value_xj =  dict[current_class][j][xj] + 1 # laplace correction\n",
        "    count_current_class = dict[current_class][\"total_count\"] + len(dict[current_class][j].keys()) #laplace correction\n",
        "    current_p =  np.log(count_current_class_with_value_xj) - np.log(count_current_class) \n",
        "    class_prob += current_p\n",
        "  \n",
        "  # output = feature_prob * class_prob\n",
        "\n",
        "  return class_prob\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyAprKbNVKcF"
      },
      "source": [
        "\n",
        "def find(dict , x) :\n",
        "  classes =  dict.keys()\n",
        "  best_prob = -1000\n",
        "  best_class = -1\n",
        "\n",
        "  for current_class in classes :\n",
        "    if (current_class == \"total_data\") : \n",
        "      continue\n",
        "    p_current_class = probabilty(dict , x , current_class)\n",
        "    if( p_current_class  > best_prob) :\n",
        "      best_prob = p_current_class\n",
        "      best_class = current_class\n",
        "    \n",
        "  return best_class "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc3rrIC5TI7H"
      },
      "source": [
        "def predict(dict , x_test):\n",
        "  y_pred = []\n",
        "  for x in x_test : \n",
        "    x_class = find(dict , x)\n",
        "    y_pred.append(x_class)\n",
        "  return y_pred "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI3Z8KcSeqt2"
      },
      "source": [
        "okay now lets test our code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjU6_PMGdsQk"
      },
      "source": [
        "def makeLabelled(column):\n",
        "    second_limit = column.mean()\n",
        "    first_limit = 0.5 * second_limit\n",
        "    third_limit = 1.5*second_limit\n",
        "    for i in range (0,len(column)):\n",
        "        if (column[i] < first_limit):\n",
        "            column[i] = 0\n",
        "        elif (column[i] < second_limit):\n",
        "            column[i] = 1\n",
        "        elif(column[i] < third_limit):\n",
        "            column[i] = 2\n",
        "        else:\n",
        "            column[i] = 3\n",
        "    return column"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKUIeZpiewVr"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "Y = iris.target"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1DOrgy6eyKB"
      },
      "source": [
        "for i in range(0,X.shape[-1]):\n",
        "    X[:,i] = makeLabelled(X[:,i])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpug1E_ceyu3"
      },
      "source": [
        "from sklearn import model_selection\n",
        "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW2a-V10e1dm",
        "outputId": "59304691-0351-4fd6-e436-5b62744d2c86"
      },
      "source": [
        "dictionary = fit(X_train,Y_train)\n",
        "\n",
        "Y_pred = predict(dictionary,X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(Y_test,Y_pred))\n",
        "print(confusion_matrix(Y_test,Y_pred))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        13\n",
            "           1       0.00      0.00      0.00        16\n",
            "           2       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.34        38\n",
            "   macro avg       0.11      0.33      0.17        38\n",
            "weighted avg       0.12      0.34      0.17        38\n",
            "\n",
            "[[13  0  0]\n",
            " [16  0  0]\n",
            " [ 9  0  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucqo0-GifgaS"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}